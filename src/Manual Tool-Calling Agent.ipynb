{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build a Tool Calling Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **1** hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can interact with external systems. You'll learn how to create custom tools that enable an LLM to perform specific actions, from extracting video IDs to fetching YouTube transcripts and metadata. Through hands-on examples, you'll first implement manual tool calling to understand the underlying mechanics, then build a flexible YouTube interaction system that can search videos, extract transcripts, fetch trending content, and generate summaries. By the end of this lab, you'll understand how to construct both fixed-sequence and recursive tool-calling chains, allowing your AI assistants to dynamically decide which tools to use and when to use them, creating truly intelligent agents that can reason about and interact with the world around them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "   <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "   <li>\n",
    "       <a href=\"#Setup\">Setup</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "           <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Tools\">Tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-video-ID-extraction-tool\">Defining video ID extraction tool</a></li>\n",
    "           <li><a href=\"#Tool-list\">Tool list</a></li>\n",
    "           <li><a href=\"#Defining-transcript-fetching-tool\">Defining transcript fetching tool</a></li>\n",
    "           <li><a href=\"#Defining-YouTube-search-tool\">Defining YouTube search tool</a></li>\n",
    "           <li><a href=\"#Defining-metadata-extraction-tool\">Defining metadata extraction tool</a></li>\n",
    "           <li><a href=\"#Defining-trending-videos-tool\">Defining trending videos tool</a></li>\n",
    "           <li><a href=\"#Defining-thumbnail-retrieval-tool\">Defining thumbnail retrieval tool</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Binding-tools\">Binding tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#How-the-LLM-calls-a-tool\">How the LLM calls a tool</a></li>\n",
    "           <li><a href=\"#LangChain-tool-binding-process\">LangChain tool binding process</a></li>\n",
    "           <li><a href=\"#Extracting-tool-call-information\">Extracting tool call information</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Automating-the-tool-calling-process\">Automating the tool calling process</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Building-the-summarization-chain\">Building the summarization chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Recursive-chain-flow\">Recursive chain flow</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-the-core-processing-logic\">Defining the core processing logic</a></li>\n",
    "           <li><a href=\"#Building-the-complete-universal-chain\">Building the complete universal chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "</ol>\n",
    "\n",
    "<li><a href=\"#Exercise\">Exercise</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Create custom tools that extend the capabilities of language models\n",
    "- Build both manual and automated tool calling chains\n",
    "- Implement recursive tool calling for dynamic, multi-step operations\n",
    "- Develop AI agents that can interact with YouTube's content programmatically\n",
    "- Apply tool calling techniques to extract, process, and summarize information from external sources\n",
    "- Design flexible workflows that allow LLMs to reason about when and how to use available tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`pytube`](https://pytube.io/en/latest/) for accessing YouTube videos and their metadata programmatically.\n",
    "*   [`youtube-transcript-api`](https://github.com/jdepoix/youtube-transcript-api) for fetching transcripts from YouTube videos.\n",
    "*   [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building tool-enabled LLM applications.\n",
    "*   [`langchain-community`](https://python.langchain.com/docs/integrations/providers/) for additional LangChain integrations.\n",
    "*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting to OpenAI's language models.\n",
    "*   [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) for enhanced YouTube data extraction capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pytube \n",
    "%pip install youtube-transcript-api==1.1.0\n",
    "%pip install langchain-community==0.3.16\n",
    "%pip install langchain==0.3.23\n",
    "%pip install langchain-openai==0.3.14\n",
    "%pip install yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pytube import YouTube\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import display, JSON\n",
    "import yt_dlp\n",
    "from typing import List, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress pytube errors\n",
    "import logging\n",
    "pytube_logger = logging.getLogger('pytube')\n",
    "pytube_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress yt-dlp warnings\n",
    "yt_dpl_logger = logging.getLogger('yt_dlp')\n",
    "yt_dpl_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API key. This lab uses the `init_chat_model` function from `langchain`. To use the model you must set the environment variable `OPENAI_API_KEY` to your OpenAI API key. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating custom tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blooks a  tool, consider the following tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "\n",
    "## Key components\n",
    "\n",
    "1. **@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "2. **Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "3. **Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "4. **Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Critical for tool selection by the LLM\n",
    "\n",
    "5. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n",
    "\n",
    "\n",
    "### Defining video ID extraction tool\n",
    "\n",
    "Now you'll define a function `extract_video_id` by denoting it as a tool that will help you to extract the video ID from a given URL. This is necessary because many YouTube API operations, including transcript extraction, require the video ID rather than the complete URL. The function uses regular expressions to handle different YouTube URL formats (standard, shortened, and embedded) and extract the 11-character video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the 11-character YouTube video ID from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): A YouTube URL containing a video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted video ID or error message if parsing fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex pattern to match video IDs\n",
    "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator wraps your function, adding those attributes (.name, .description, .func) and registering it with LangChain's tool system. The original function becomes accessible through the .func attribute, but the overall object is an instance of LangChain's tool class, with additional methods like .run() for direct invocation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Testing the video ID extraction tool\n",
    "\n",
    "\n",
    "Now you'll be testing your `extract_video_id` tool to verify that it's correctly registered with LangChain. These print statements will show you:\n",
    "1. The tool's name (as it will be referenced by the LLM)\n",
    "2. The tool's description (which helps the LLM understand when to use this tool)\n",
    "3. The actual function reference that will be called\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n",
      "----------------------------\n",
      "Extracts the 11-character YouTube video ID from a URL.\n",
      "\n",
      "Args:\n",
      "    url (str): A YouTube URL containing a video ID.\n",
      "\n",
      "Returns:\n",
      "    str: Extracted video ID or error message if parsing fails.\n",
      "----------------------------\n",
      "<function extract_video_id at 0x7ab84342da80>\n"
     ]
    }
   ],
   "source": [
    "print(extract_video_id.name)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.description)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing tool execution\n",
    "\n",
    "Here, you're testing the actual execution of your `extract_video_id` tool with a real YouTube URL. You can call the tool using the `.run()` method, which is a convenient way to execute the tool directly and see its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hfIUstzHs9A'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id.run(\"https://www.youtube.com/watch?v=hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x7ab84342da80>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows that your function has been transformed into a `StructuredTool` object by LangChain. It displays the tool's name ('extract_video_id'), its description (our docstring), a Pydantic schema for input validation, and a reference to your original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool list \n",
    "Multiple tools will be created to enhance the LLM's capabilities. For organization, create a list called tools, which is a standard Python list that contains tool objects created with the @tool decorator. This list doesn't execute functions or determine call order - it simply collects tool objects in one place so they can be efficiently passed to the language model via llm.bind_tools(tools). This approach allows the LLM to access all available tools without requiring them to be individually registered.\n",
    "\n",
    "Adding the ```extract_video_id``` tool to your tools list, which you can later provide to the LLM so it can use this functionality when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x7ab84342da80>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = []\n",
    "tools.append(extract_video_id)\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have understood the basic structure, let's define the rest of the tools you'll need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transcript fetching tool\n",
    "\n",
    "Now you're going to create another tool that fetches the transcript from a YouTube video. This tool uses the `YouTubeTranscriptApi` library to retrieve the captions or subtitles from a video. You'll be taking the video ID (which can be extracted using your previous tool) and an optional language parameter. The function attempts to get the transcript and joins all text segments into a continuous string, or returns an error message if the transcript can't be retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
    "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcript text or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
    "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the fetch_transcript tool by directly calling it with the .run() method on a specific video ID. This will attempt to retrieve the transcript for the video with ID \"hfIUstzHs9A\" in the default English language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm. Whether it\\'s writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule. I\\'m a senior manager of business strategy at IBM Research, and today I\\'m going to give a brief overview of this new field of AI that\\'s emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task. They predicted that we were going to start moving to a new paradigm, where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions is that it\\'s been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means, in the language domain, is basically I\\'ll feed a bunch of sentences-- and I\\'m talking terabytes of data here --to train this model. And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\". And I\\'m trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it\\'s this generative capability of the model-- predicting and generating the next word --based off of previous words that it\\'s seen beforehand, that is why that foundation models are actually a part of the field of AI called generative AI because we\\'re generating something new in this case, the next word in a sentence. And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models, and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or named-entity recognition --things that you don\\'t normally associate as being a generative-based model or capability. And this process is called tuning. Where you can tune your foundation model by introducing a small amount of data, you update the parameters of your model and now perform a very specific natural language task. If you don\\'t have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment? The model\\'s going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem, which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data. Again, data with a capital D-- terabytes of data --that by the time that they\\'re applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute cost. So that penalty for having this model see so much data is that they\\'re very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They\\'re also expensive-- by the time they get to a huge size, a couple billion parameters --they\\'re also very expensive to run inference. You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. So just like data is a huge advantage for these models, they\\'ve seen so much unstructured data, it also comes at a cost, especially in the domain like language. A lot of these models are trained basically off of language data that\\'s been scraped from the Internet. And there\\'s so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn\\'t be able to go through and actually vet every single data point to make sure that it wasn\\'t biased and didn\\'t contain hate speech or other toxic information. And that\\'s just assuming you actually know what the data is. Often we don\\'t even know-- for a lot of these open source models that have been posted --what the exact datasets are that these models have been trained on leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. But my partners in IBM Research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I\\'ve talked through so far have just been on the language side. But the reality is, there are a lot of other domains that foundation models can be applied towards. Famously, we\\'ve seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that\\'s then used to generate a custom image. We\\'ve seen models for code with products like Copilot that can help complete code as it\\'s being authored. And IBM\\'s innovating across all of these domains. So whether it\\'s language models that we\\'re building into products like Watson Assistant and Watson Discovery, vision models that we\\'re building into products like Maximo Visual Inspection, or Ansible code models that we\\'re building with our partners at Red Hat under Project Wisdom. We\\'re innovating across all of these domains and more. We\\'re working on chemistry. So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics. And we\\'re working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. If you\\'re interested in learning more, particularly how IBM is working to improve some of these disadvantages, making foundation models more trustworthy and more efficient, please take a look at the links below. Thank you.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript.run(\"hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Adding the `fetch_transcript` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(fetch_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining YouTube search tool\n",
    "\n",
    "Now let's create a search tool that allows finding videos on YouTube based on a query string. This tool uses the `Search` class from the PyTube library to perform searches on YouTube. When given a search term, it returns a list of matching videos with each video represented as a dictionary containing the title, video ID, and a shortened URL. This tool will be helpful for discovering relevant videos when you don't already have a specific URL in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import Search\n",
    "from langchain.tools import tool\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def search_youtube(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search YouTube for videos matching the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search term to look for on YouTube\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing video titles and IDs in format:\n",
    "        [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n",
    "        Returns error message if search fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Search(query)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": yt.title,\n",
    "                \"video_id\": yt.video_id,\n",
    "                \"url\": f\"https://youtu.be/{yt.video_id}\"\n",
    "            }\n",
    "            for yt in s.results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `search_youtube` tool by calling it with the `.run()` method and the search query \"Generative AI.\" This will return a list of YouTube videos related to generative AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "title": "Generative AI explained in 2 minutes",
        "url": "https://youtu.be/rwF-X5STYks",
        "video_id": "rwF-X5STYks"
       },
       {
        "title": "Generative AI in a Nutshell - how to survive and thrive in the age of AI",
        "url": "https://youtu.be/2IK3DFHRFfw",
        "video_id": "2IK3DFHRFfw"
       },
       {
        "title": "Generative AI Full Course ‚Äì Gemini Pro, OpenAI, Llama, Langchain, Pinecone, Vector Databases & More",
        "url": "https://youtu.be/mEsleV16qdo",
        "video_id": "mEsleV16qdo"
       },
       {
        "title": "Generative AI Explained In 5 Minutes | What Is GenAI? | Introduction To Generative AI | Simplilearn",
        "url": "https://youtu.be/NRmAXDWJVnU",
        "video_id": "NRmAXDWJVnU"
       },
       {
        "title": "Introduction to Generative AI",
        "url": "https://youtu.be/G2fqAlgmoPo",
        "video_id": "G2fqAlgmoPo"
       },
       {
        "title": "What is generative AI and how does it work? ‚Äì The Turing Lectures with Mirella Lapata",
        "url": "https://youtu.be/_6R7Ym6Vy_I",
        "video_id": "_6R7Ym6Vy_I"
       },
       {
        "title": "AI, Machine Learning, Deep Learning and Generative AI Explained",
        "url": "https://youtu.be/qYNweeDHiyU",
        "video_id": "qYNweeDHiyU"
       },
       {
        "title": "What's the future for generative AI? - The Turing Lectures with Mike Wooldridge",
        "url": "https://youtu.be/b76gsOSkHB4",
        "video_id": "b76gsOSkHB4"
       },
       {
        "title": "Integrating Generative AI Into Business Strategy: Dr. George Westerman",
        "url": "https://youtu.be/9RvWcXVaAng",
        "video_id": "9RvWcXVaAng"
       },
       {
        "title": "What is Generative AI?",
        "url": "https://youtu.be/pWNAtUwnBS8",
        "video_id": "pWNAtUwnBS8"
       },
       {
        "title": "Generative vs Agentic AI: Shaping the Future of AI Collaboration",
        "url": "https://youtu.be/EDb37y_MhRw",
        "video_id": "EDb37y_MhRw"
       },
       {
        "title": "Watch this before using generative AI",
        "url": "https://youtu.be/unPKJJjQP0A",
        "video_id": "unPKJJjQP0A"
       },
       {
        "title": "Variational Autoencoders | Generative AI Animated",
        "url": "https://youtu.be/qJeaCHQ1k2w",
        "video_id": "qJeaCHQ1k2w"
       },
       {
        "title": "Introduction to Generative AI",
        "url": "https://youtu.be/cZaNf2rA30k",
        "video_id": "cZaNf2rA30k"
       },
       {
        "title": "Generative AI Full Course (2026) | Gen AI Tutorial For Beginners FREE | Intellipaat",
        "url": "https://youtu.be/4Nm0sl9y2U4",
        "video_id": "4Nm0sl9y2U4"
       },
       {
        "title": "Generative AI vs. Predictive AI | Eric Siegel",
        "url": "https://youtu.be/oqgueqqli-A",
        "video_id": "oqgueqqli-A"
       },
       {
        "title": "Intro to Machine Learning featuring Generative AI",
        "url": "https://youtu.be/tmB5JIX3Lxk",
        "video_id": "tmB5JIX3Lxk"
       },
       {
        "title": "AI-900 - Learning About Generative AI",
        "url": "https://youtu.be/Ch6KE7KxHGM",
        "video_id": "Ch6KE7KxHGM"
       },
       {
        "title": "The Turing Lectures: The future of generative AI",
        "url": "https://youtu.be/2kSl0xkq2lM",
        "video_id": "2kSl0xkq2lM"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_out=search_youtube.run(\"Generative AI\")\n",
    "display(JSON(search_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending the `search_youtube` tool to tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(search_youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining metadata extraction tool\n",
    "\n",
    "Now you'll create a tool that extracts detailed metadata from a YouTube video using the `yt-dlp` library. This tool takes a YouTube URL and returns comprehensive information about the video, including its title, view count, duration, channel name, like count, comment count, and any chapter markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_full_metadata(url: str) -> dict:\n",
    "    \"\"\"Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.\"\"\"\n",
    "    with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        return {\n",
    "            'title': info.get('title'),\n",
    "            'views': info.get('view_count'),\n",
    "            'duration': info.get('duration'),\n",
    "            'channel': info.get('uploader'),\n",
    "            'likes': info.get('like_count'),\n",
    "            'comments': info.get('comment_count'),\n",
    "            'chapters': info.get('chapters', [])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_full_metadata` tool by running it on a specific YouTube video URL. This will extract comprehensive information about the video with ID \"qWHaMrR5WHQ\" without downloading the actual video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "channel": "Praveen Reddy Learnings",
       "chapters": null,
       "comments": 5,
       "duration": 1048,
       "likes": 24,
       "title": "Explained how to use Tools (tool calling) in LangChain",
       "views": 1437
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_data=get_full_metadata.run(\"https://youtu.be/qWHaMrR5WHQ\")\n",
    "display(JSON(meta_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the `get_full_metadata` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_full_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining trending videos tool\n",
    "\n",
    "Now, you'll create a tool to fetch the currently trending videos on YouTube for a specific region. This tool uses `yt-dlp` to access YouTube's trending feed based on a provided country code (like \"US\" for the United States or \"IN\" for India). It collects important information about each trending video, including the title, video ID, URL, channel name, duration, and view count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def get_trending_videos(region_code: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches currently trending YouTube videos for a specific region.\n",
    "    \n",
    "    Args:\n",
    "        region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with video details: title, video_id, channel, view_count, duration\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'geo_bypass_country': region_code.upper(),\n",
    "        'extract_flat': True,\n",
    "        'quiet': True,\n",
    "        'force_generic_extractor': True,\n",
    "        'logger': yt_dpl_logger\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(\n",
    "                f\"ytsearchdate25:trending in {region_code}\",\n",
    "                download=False\n",
    "            )\n",
    "            trending_videos = []\n",
    "            for entry in info['entries']:\n",
    "                video_data = {\n",
    "                    'title': entry.get('title', 'N/A'),\n",
    "                    'video_id': entry.get('id', 'N/A'),\n",
    "                    'url': entry.get('url', 'N/A'),\n",
    "                    'channel': entry.get('uploader', 'N/A'),\n",
    "                    'duration': entry.get('duration', 0),\n",
    "                    'view_count': entry.get('view_count', 0)\n",
    "                }\n",
    "                trending_videos.append(video_data)\n",
    "                \n",
    "            return trending_videos[:25]  # Return top 25 trending videos\n",
    "            \n",
    "    except Exception as e:\n",
    "        return [{'error': f\"Failed to fetch trending videos: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_trending_videos` tool by running it with the region code `\"US\"` to fetch trending videos from the United States. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "channel": "SIKARI BHAI",
        "duration": 17,
        "title": "id hack #shorts #trending #freefire #viralshorts #youtubeshorts #popularshorts #freefireshorts #ff",
        "url": "https://www.youtube.com/shorts/lQyj8x9nZ5w",
        "video_id": "lQyj8x9nZ5w",
        "view_count": 0
       },
       {
        "channel": "Aman___up___ 42 ",
        "duration": 16,
        "title": "ü•∞üò¢‚ù§Ô∏è‚Äçü©π#trending sad song my channel ID ko subscribe kare please üôè",
        "url": "https://www.youtube.com/shorts/IRR09-fxP_c",
        "video_id": "IRR09-fxP_c",
        "view_count": 0
       },
       {
        "channel": "Abhi_Adiwal_Gaming",
        "duration": 15,
        "title": "THIS FOR MY ID üóø #trending #freefire #totalgaming",
        "url": "https://www.youtube.com/shorts/KB9egJJyQYU",
        "video_id": "KB9egJJyQYU",
        "view_count": 0
       },
       {
        "channel": "JAFRU14 GAMING ",
        "duration": 37,
        "title": "Ôøº Papa's real ID will be banned ü§£üòÇ #funny #freefire #trending #comedy",
        "url": "https://www.youtube.com/shorts/SS57hClKwAU",
        "video_id": "SS57hClKwAU",
        "view_count": 0
       },
       {
        "channel": "Life of Yusuf ",
        "duration": 54,
        "title": "Email ID is Significant üá¨üáß | #uk #india #vlog #minivlog #viralvideo #trending #fyp",
        "url": "https://www.youtube.com/shorts/yJMQEX0wC-A",
        "video_id": "yJMQEX0wC-A",
        "view_count": 17
       },
       {
        "channel": "Viram Zala Shorts",
        "duration": 39,
        "title": "50.k follower Instagram ID complete #trending #youtubeshorts #trend #shortvideo #ytshorts",
        "url": "https://www.youtube.com/shorts/nwZwkEUB2yw",
        "video_id": "nwZwkEUB2yw",
        "view_count": 437
       },
       {
        "channel": "Ayush Bhandari ",
        "duration": 30,
        "title": "Lokesh gaming 10000000 for free fire id üî• #shortvideo #viralvideo #shortsfeed #trending #shorts",
        "url": "https://www.youtube.com/shorts/9A7HCpZvuuo",
        "video_id": "9A7HCpZvuuo",
        "view_count": 271
       },
       {
        "channel": "AYAN LIVE FF",
        "duration": 11,
        "title": "Bot id me m4o max emote shocked brother üíÄ#freefire #shorts #trending",
        "url": "https://www.youtube.com/shorts/N4M2YKjkYvU",
        "video_id": "N4M2YKjkYvU",
        "view_count": 0
       },
       {
        "channel": "DEVA JI BOLTE ",
        "duration": 14,
        "title": "Just A NORMAL ID ü§´ü§´ü§´ #freefire #youtubeshorts #viral  #trendingshorts #trending",
        "url": "https://www.youtube.com/shorts/sqIALwdyYf0",
        "video_id": "sqIALwdyYf0",
        "view_count": 191
       },
       {
        "channel": "@Raj_ff2M",
        "duration": 12,
        "title": "1200000 ki ID #viral #freefire #trending #reels #shortsviral #shorts #shortvideo",
        "url": "https://www.youtube.com/shorts/0VtEFT3JVQc",
        "video_id": "0VtEFT3JVQc",
        "view_count": 0
       },
       {
        "channel": "POWERVIBES OFFICIAL",
        "duration": 15,
        "title": "mala mala bloopersüòÇüôèüèª IG ID uppum.mulakumm #shorts #viral #trending #shortvideo #viralvideo #short",
        "url": "https://www.youtube.com/shorts/pX-wtpulOl4",
        "video_id": "pX-wtpulOl4",
        "view_count": 691
       },
       {
        "channel": "BR ANKIT  GAMES",
        "duration": 11,
        "title": "OLD ID HAA ID BANü•πüí´.. #lod #freefire #ban #viral #trending #shots #video",
        "url": "https://www.youtube.com/shorts/9811ObZul3Y",
        "video_id": "9811ObZul3Y",
        "view_count": 0
       },
       {
        "channel": "pahlevi malik",
        "duration": 23,
        "title": "#shorts #youtubeshorts #trending #viralshorts #shortsvideo. https://id.shp.ee/xzzq389?smtt=0.0.9",
        "url": "https://www.youtube.com/watch?v=18MceZk-13M",
        "video_id": "18MceZk-13M",
        "view_count": 1037
       },
       {
        "channel": "Ha_Donya!",
        "duration": 7,
        "title": "//trend ID:not mine!!// #edit #–∑–∞–ª–∏—Ç–∏ #subwaysurfs #–ø–µ—Ä–µ–ø–∏—Å–∫–∞ #drawing #art",
        "url": "https://www.youtube.com/shorts/tDgKRYsQAuo",
        "video_id": "tDgKRYsQAuo",
        "view_count": 0
       },
       {
        "channel": "Sky Gamer143",
        "duration": 16,
        "title": "Free fire ID Bandüò¢üò¢ #freefire #garena #garenafreefire #trending #trendingshorts",
        "url": "https://www.youtube.com/shorts/N7zoCq7jE3Q",
        "video_id": "N7zoCq7jE3Q",
        "view_count": 1575
       },
       {
        "channel": "PANKAJ STAR 100k",
        "duration": 24,
        "title": "free fire ID seller üì±#shorts #viral #trending",
        "url": "https://www.youtube.com/shorts/_lRRb23bZOc",
        "video_id": "_lRRb23bZOc",
        "view_count": 0
       },
       {
        "channel": "@ tacnicalai",
        "duration": 40,
        "title": "50 ki id #trending  #viralvideo  #lifeisbutadream  #gaming",
        "url": "https://www.youtube.com/shorts/mCl7uDMk6Qo",
        "video_id": "mCl7uDMk6Qo",
        "view_count": 0
       },
       {
        "channel": "official vijay Rox 2.0",
        "duration": 15,
        "title": "üòÇInstagram IDüòÇ #comedy #trending #entertainment #shortvideos #viralshort #reelsshorts #funny #shorts",
        "url": "https://www.youtube.com/shorts/sGFaa3Ocy6Y",
        "video_id": "sGFaa3Ocy6Y",
        "view_count": 0
       },
       {
        "channel": "VIP GAMING LIVE ",
        "duration": 13,
        "title": "Id Blacklist Ho Gaya üòû #freefire #freefireclips #freefirehighlights #trending #freefireshorts",
        "url": "https://www.youtube.com/shorts/HS3jTlW-_1U",
        "video_id": "HS3jTlW-_1U",
        "view_count": 0
       },
       {
        "channel": "EMISHA üíó",
        "duration": 71,
        "title": "Noob ki id üÜî #shorts #freefire #viralvideo #trending #youtubeshortsvideo #youtubeshorts",
        "url": "https://www.youtube.com/shorts/gNQsG_4649U",
        "video_id": "gNQsG_4649U",
        "view_count": 0
       },
       {
        "channel": "SofiX Lil_mao",
        "duration": 22,
        "title": "üç°!! √∞√ª trendüç•!! id not mine! #art #dandysworld #glourdy #roblox #drawing #paperanimation #meme",
        "url": "https://www.youtube.com/shorts/adGg4gC4tW4",
        "video_id": "adGg4gC4tW4",
        "view_count": 847
       },
       {
        "channel": "Banko",
        "duration": 20,
        "title": "#trending #id #sell ID",
        "url": "https://www.youtube.com/shorts/zE2w4JLC_rY",
        "video_id": "zE2w4JLC_rY",
        "view_count": 0
       },
       {
        "channel": "AMAN GAMING 8M",
        "duration": 14,
        "title": "For id sall #freefire #trending #bayralshort #shortvideo",
        "url": "https://www.youtube.com/shorts/SLUtTU6wamA",
        "video_id": "SLUtTU6wamA",
        "view_count": 899
       },
       {
        "channel": "Ipehhh",
        "duration": 20,
        "title": "ID Capcut: 3418818242 #template #trending #pret #padawaktunya",
        "url": "https://www.youtube.com/shorts/ekx1BrwnPbs",
        "video_id": "ekx1BrwnPbs",
        "view_count": 409
       },
       {
        "channel": "SHADOW QUEEN",
        "duration": 40,
        "title": "MYSTERIOUS ID üò± IN FREE FIRE üî• #freefire #shorts #shortsfeed #shadowqueen #trending #viral",
        "url": "https://www.youtube.com/shorts/xKLgsMqLuGI",
        "video_id": "xKLgsMqLuGI",
        "view_count": 1085
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trending_videos=get_trending_videos.run(\"ID\")\n",
    "# Display as formatted JSON\n",
    "display(JSON(trending_videos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_trending_videos` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_trending_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining thumbnail retrieval tool\n",
    "\n",
    "Now you'll create a tool to extract all available thumbnail images for a YouTube video. This tool uses `yt-dlp` to retrieve information about the various thumbnail images that YouTube generates for videos at different resolutions. For each thumbnail, collect its URL, width, height, and formatted resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_thumbnails(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get available thumbnails for a YouTube video using its URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): YouTube video URL (any format)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with thumbnail URLs and resolutions in YouTube's native order\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            \n",
    "            thumbnails = []\n",
    "            for t in info.get('thumbnails', []):\n",
    "                if 'url' in t:\n",
    "                    thumbnails.append({\n",
    "                        \"url\": t['url'],\n",
    "                        \"width\": t.get('width'),\n",
    "                        \"height\": t.get('height'),\n",
    "                        \"resolution\": f\"{t.get('width', '')}x{t.get('height', '')}\".strip('x')\n",
    "                    })\n",
    "            \n",
    "            return thumbnails\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Failed to get thumbnails: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_thumbnails` tool by running it on a specific YouTube video URL. This will extract information about all available thumbnail images for the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/default.jpg",
        "width": null
       },
       {
        "height": 90,
        "resolution": "120x90",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/default.webp",
        "width": 120
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mqdefault.jpg",
        "width": null
       },
       {
        "height": 180,
        "resolution": "320x180",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mqdefault.webp",
        "width": 320
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/0.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/0.webp",
        "width": null
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDK33nNmNAuVa1ibN7uTkq5okdwfw",
        "width": 168
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCKgBEF5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLBQiFoh8Nia1_XrFTBE3-g4hp61xQ",
        "width": 168
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCMQBEG5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBa9uAOnYdtfae6yOMFyYfUFEvjkA",
        "width": 196
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCMQBEG5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLDw2g-OpL84ZvPbhVdQ9S_Unp3RMg",
        "width": 196
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCPYBEIoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLDjZI5FIYKE2PlZhm9PQTk0eKeDAw",
        "width": 246
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCPYBEIoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCJk1-Lqfb8XHsFyiBDiCN0opMcDA",
        "width": 246
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLBTNOqMuqjK5oJ-_mtGsUJOh-sVkA",
        "width": 336
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCNACELwBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAlaeOG0AijM70t7qsBkkaEvRb3FQ",
        "width": 336
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg",
        "width": 480
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hqdefault.webp",
        "width": 480
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sddefault.jpg",
        "width": null
       },
       {
        "height": 480,
        "resolution": "640x480",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sddefault.webp",
        "width": 640
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq720.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq720.webp",
        "width": null
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/maxresdefault.jpg",
        "width": 1920
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/maxresdefault.webp",
        "width": 1920
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thumbnails=get_thumbnails.run(\"https://www.youtube.com/watch?v=qWHaMrR5WHQ\")\n",
    "\n",
    "display(JSON(thumbnails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_thumbnails` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_thumbnails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Binding tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, you'll bind your collection of tools to the language model. It enables the LLM to access and use your custom YouTube tools during conversations. By binding the tools, you're giving the model the ability to call these functions when it determines they're needed to fulfill a user request, making the LLM aware of your tools' capabilities and how to use them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```bind_tools()``` function passes all this information to the language model. It converts each tool's attributes (name, description, parameters schema) into a standardized format that the LLM can understand and use to determine when and how to call specific tools based on user requests. Similar to the following code where the schema for each tool is stored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
       "name": "extract_video_id",
       "parameters": {
        "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "extract_video_id",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
       "name": "fetch_transcript",
       "parameters": {
        "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
        "properties": {
         "language": {
          "default": "en",
          "title": "Language",
          "type": "string"
         },
         "video_id": {
          "title": "Video Id",
          "type": "string"
         }
        },
        "required": [
         "video_id"
        ],
        "title": "fetch_transcript",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
       "name": "search_youtube",
       "parameters": {
        "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
        "properties": {
         "query": {
          "title": "Query",
          "type": "string"
         }
        },
        "required": [
         "query"
        ],
        "title": "search_youtube",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
       "name": "get_full_metadata",
       "parameters": {
        "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_full_metadata",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
       "name": "get_trending_videos",
       "parameters": {
        "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
        "properties": {
         "region_code": {
          "title": "Region Code",
          "type": "string"
         }
        },
        "required": [
         "region_code"
        ],
        "title": "get_trending_videos",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
       "name": "get_thumbnails",
       "parameters": {
        "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_thumbnails",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    schema = {\n",
    "   \"name\": tool.name,\n",
    "   \"description\": tool.description,\n",
    "   \"parameters\": tool.args_schema.schema() if tool.args_schema else {},\n",
    "   \"return\": tool.return_type if hasattr(tool, \"return_type\") else None}\n",
    "    display(JSON(schema))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the LLM calls a tool\n",
    "\n",
    "Now, define a sample user query that asks for a summary of a specific YouTube video. This query will be used to demonstrate how your LLM can understand a natural language request and use the appropriate tools you've provided to fulfill it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating a message object to represent your user query. You'll be wrapping the query string in a HumanMessage object, which is the standard way to format user inputs in LangChain. It represents a human message as a person is expected to initiate the interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content = query)]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain tool binding process\n",
    "\n",
    "This step involves sending your message to the LLM and storing its response. Here you'll invoke the language model with your user query about summarizing a YouTube video. The response will contain both text content and potentially tool calls that the model decides to make. ``response_1`` contains the LLM's response to the user message, including any tool calls it decides to make. The response object contains the content of the LLM's reply plus structured information about which tools it wants to call and with what parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_52e4IGkcwevYejgibaSH7ZRO', 'function': {'arguments': '{\"url\":\"https://www.youtube.com/watch?v=T-D1OfcDW1M\"}', 'name': 'extract_video_id'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 457, 'total_tokens': 486, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZzrHPFmaB7cawELMampSOdyx2Wcm', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bf53fe79-ca6c-4680-bf66-164e24d077f5-0', tool_calls=[{'name': 'extract_video_id', 'args': {'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}, 'id': 'call_52e4IGkcwevYejgibaSH7ZRO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 457, 'output_tokens': 29, 'total_tokens': 486, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(messages)\n",
    "response_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tool call information\n",
    "After receiving the LLM's response, you need to extract the structured tool call information. The line tool_calls_1 = response_1.tool_calls gets the tool call objects that contain which tool the LLM has decided to use and what parameters to pass to it. This information will be used to execute the appropriate tool with the correct inputs.\n",
    "\n",
    "#### Creating a tool mapping dictionary\n",
    "\n",
    "Now you'll create a dictionary that maps tool names to their corresponding function objects. This mapping will be useful later when you need to programmatically invoke specific tools based on their names. It allows you to easily look up and execute a tool function when you have only the tool name as a string, which will be important when processing tool calls from the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"get_thumbnails\" : get_thumbnails,\n",
    "    \"get_trending_videos\": get_trending_videos,\n",
    "    \"extract_video_id\": extract_video_id,\n",
    "    \"fetch_transcript\": fetch_transcript,\n",
    "    \"search_youtube\": search_youtube,\n",
    "    \"get_full_metadata\": get_full_metadata\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's response. When the LLM determines it needs to use one of your tools, it includes structured \"tool_calls\" in its response. Here, you're accessing those tool calls to see which tools the model decided to use in order to fulfill the request about summarizing the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "args": {
         "url": "https://www.youtube.com/watch?v=T-D1OfcDW1M"
        },
        "id": "call_52e4IGkcwevYejgibaSH7ZRO",
        "name": "extract_video_id",
        "type": "tool_call"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "display(JSON(tool_calls_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you're seeing the structure of the tool call that the LLM decided to make. The tool call is formatted as a dictionary with the following key components:\n",
    "\n",
    "1. `name`: 'extract_video_id' - This identifies which tool the LLM wants to use first (the video ID extraction tool)\n",
    "2. `args`: Contains the arguments to pass to the tool - in this case, the YouTube URL from your query\n",
    "3. `id`: A unique identifier for this specific tool call, which helps track the request/response pair\n",
    "4. `type`: Indicates this is a tool call rather than other types of AI responses\n",
    "\n",
    "This shows that the LLM correctly understood it needs to first extract the video ID from the URL before it can proceed with summarizing the video content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the name of the first tool that the LLM decided to use. Here you're extracting just the name component `('extract_video_id')` from the first tool call in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n"
     ]
    }
   ],
   "source": [
    "tool_name=tool_calls_1[0]['name']\n",
    "print(tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a tool ID to help the LLM know where the output came from:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_52e4IGkcwevYejgibaSH7ZRO\n"
     ]
    }
   ],
   "source": [
    "tool_call_id =tool_calls_1[0]['id']\n",
    "print(tool_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the arguments that need to be passed to the chosen tool. Here, you're extracting the arguments component from the first tool call, which contains the YouTube URL that needs to be processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}\n"
     ]
    }
   ],
   "source": [
    "args=tool_calls_1[0]['args']\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), you append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the tool call that the LLM requested. Here, you're using your tool mapping dictionary to:\n",
    "1. Look up the appropriate function based on the tool name ('extract_video_id')\n",
    "2. Call that function with the arguments provided by the LLM\n",
    "3. Capture the output (the extracted video ID)\n",
    "\n",
    "This shows how you can programmatically execute the tools that the LLM decided to use. First, you get the tool from ```tool_mapping```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x7ab84342da80>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tool=tool_mapping[tool_calls_1[0]['name']]\n",
    "my_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll then call the tool with the arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-D1OfcDW1M'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id =my_tool.invoke(tool_calls_1[0]['args'])\n",
    "video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the tool's output to your conversation history. You'll create a `ToolMessage` that contains:\n",
    "1. The result from executing the tool (the extracted video ID)\n",
    "2. The original tool call ID to link this response back to the specific request\n",
    "\n",
    "By appending this message to your conversation history, you're informing the LLM about the results of the tool execution, which it can use in its next response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = video_id, tool_call_id = tool_calls_1[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_og4wR9BLJPPeNFaJKz8pGwa5', 'function': {'arguments': '{\"video_id\":\"T-D1OfcDW1M\",\"language\":\"en\"}', 'name': 'fetch_transcript'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 503, 'total_tokens': 530, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZzrJrVhnMePfEkwL2GMCxIYVOp1e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--588c5a60-70f2-4413-9498-b35e95406932-0', tool_calls=[{'name': 'fetch_transcript', 'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'}, 'id': 'call_og4wR9BLJPPeNFaJKz8pGwa5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 503, 'output_tokens': 27, 'total_tokens': 530, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = llm_with_tools.invoke(messages)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "channel": "God Says 11:11",
        "duration": 4,
        "title": "Dua said, ‚ÄúPeople want us to compete‚Ä¶ but the truth #dualipa #trending #shortsvideo",
        "url": "https://www.youtube.com/shorts/PitOVyLdT7w",
        "video_id": "PitOVyLdT7w",
        "view_count": 0
       },
       {
        "channel": "Md Qamar Raza",
        "duration": 46,
        "title": "#hiphop #rap #music #trending #freestyle #trendingshorts #trend#us #usa #canada #france",
        "url": "https://www.youtube.com/shorts/xNU_BOOqqQw",
        "video_id": "xNU_BOOqqQw",
        "view_count": 0
       },
       {
        "channel": "Harshal_GREEN SCREEN ",
        "duration": 16,
        "title": "us Gym workout üí™#shorts #shortsviral #trending #shortsfeed",
        "url": "https://www.youtube.com/shorts/GgZj1JlVQN8",
        "video_id": "GgZj1JlVQN8",
        "view_count": 0
       },
       {
        "channel": "Sribash Defence Vlogs",
        "duration": 28,
        "title": "Us Ladki Per Dil Aaya Hai ##trending  #youtubevideo",
        "url": "https://www.youtube.com/shorts/3vQ_Sou5slA",
        "video_id": "3vQ_Sou5slA",
        "view_count": 0
       },
       {
        "channel": "Anshikazone",
        "duration": 10,
        "title": "#popular #trending #viral #famous #snapchat #friends #nadia#trend#us",
        "url": "https://www.youtube.com/shorts/Ebya8wHySZk",
        "video_id": "Ebya8wHySZk",
        "view_count": 0
       },
       {
        "channel": "lofty._.aesthetic ",
        "duration": 15,
        "title": "Us core üåö #trending #viral #shorts",
        "url": "https://www.youtube.com/shorts/KXmyAVZG4Bs",
        "video_id": "KXmyAVZG4Bs",
        "view_count": 0
       },
       {
        "channel": "ùóïùóºùòÄùòÄ_ùóïùóÆùóØùòÜüö©",
        "duration": 11,
        "title": "Meanwhile US:‚ò†Ô∏è  @Beautyblaxk._.X #popular #trending #fyp„Ç∑„Çöviral #recommended #bossbaby",
        "url": "https://www.youtube.com/shorts/oBagXFelNNU",
        "video_id": "oBagXFelNNU",
        "view_count": 137
       },
       {
        "channel": "Naina Choudhary",
        "duration": 16,
        "title": "us mood se bhi hum guzre hai üñ§üé∂#trending #bolleywoodsong MithunChakraborty #trendingshorts",
        "url": "https://www.youtube.com/shorts/C4cFv9Ri8_U",
        "video_id": "C4cFv9Ri8_U",
        "view_count": 0
       },
       {
        "channel": "Kamni rk",
        "duration": 15,
        "title": "Sajan Mera us paar Hai..‚ù§Ô∏èü•∞ü•Ä..#shorts #viralvideos #trending #hindisong #youtube #channel",
        "url": "https://www.youtube.com/shorts/VtKtKXfov7c",
        "video_id": "VtKtKXfov7c",
        "view_count": 0
       },
       {
        "channel": "Tuntun",
        "duration": 16,
        "title": "sajan mera us par hai #trending #ytshorts #video",
        "url": "https://www.youtube.com/shorts/9wf1Q3-0_Ho",
        "video_id": "9wf1Q3-0_Ho",
        "view_count": 0
       },
       {
        "channel": "Salman YouTube up 36 ",
        "duration": 16,
        "title": "us ladki per Dil aaya hai #trending #viral #song",
        "url": "https://www.youtube.com/shorts/tBGfLfFxHNk",
        "video_id": "tBGfLfFxHNk",
        "view_count": 0
       },
       {
        "channel": "Jungle Fruit Shorts ",
        "duration": 11,
        "title": "Let me eat feeding my pet #fruit #shorts #trending #us #ai",
        "url": "https://www.youtube.com/shorts/Ib-OdErqMPw",
        "video_id": "Ib-OdErqMPw",
        "view_count": 0
       },
       {
        "channel": "GiggleGov",
        "duration": 9,
        "title": "breaking NewsUS Army Big military peration  ‚ù§Ô∏è #trending #us #army_gs #shorts",
        "url": "https://www.youtube.com/shorts/lIrpjTyMAAo",
        "video_id": "lIrpjTyMAAo",
        "view_count": 0
       },
       {
        "channel": "DESI_GAME_PLAY",
        "duration": 13,
        "title": "#short-!! ü§£ùìñùì∏ùì≠ ùì∞ùì≤ùìøùìÆ ùìæùìº  !!ùìØùìªùìÆùìÆùìØùì≤ùìªùìÆ ùì∂ùì™ùîÅ ùìºùì±ùì∏ùìªùìΩ ùìøùì≤ùì≠ùìÆùì∏ #trending  #ùìØùìæùì∑ùì∑ùîÇ #free #2026..",
        "url": "https://www.youtube.com/shorts/44-PnnRfd98",
        "video_id": "44-PnnRfd98",
        "view_count": 0
       },
       {
        "channel": "music good vibes",
        "duration": 34,
        "title": "Let us never have prejudice against Allah SWT #ustadzadihidayat #trending",
        "url": "https://www.youtube.com/shorts/4otRqrqAJjQ",
        "video_id": "4otRqrqAJjQ",
        "view_count": 0
       },
       {
        "channel": "sparkling star cutoo",
        "duration": 18,
        "title": "#shortvideos #ytshorts #trending #love #mandala #explorenow #viral #asmr #like #foryou #usa #us #how",
        "url": "https://www.youtube.com/shorts/1AswcglVKLs",
        "video_id": "1AswcglVKLs",
        "view_count": 0
       },
       {
        "channel": "Sanjana Rawat",
        "duration": 56,
        "title": "a full üåù day with usüôà || #viralshort #minivlog #vlog #trending #viralvideo",
        "url": "https://www.youtube.com/shorts/1kYR8fa0cI4",
        "video_id": "1kYR8fa0cI4",
        "view_count": 906
       },
       {
        "channel": "Financetips",
        "duration": 7,
        "title": "BlackRock Portfolioü§ë#shorts #ytshorts #viral #portfolio #stocks #shortsfeed #blackrock #us #trending",
        "url": "https://www.youtube.com/shorts/lmyB86tUjKQ",
        "video_id": "lmyB86tUjKQ",
        "view_count": 0
       },
       {
        "channel": "ETHAN BANDA",
        "duration": 32,
        "title": "This is about that time üò∞ Garena free fire üò® trending video #ytshorts #shots",
        "url": "https://www.youtube.com/shorts/lVFkDKsaJbU",
        "video_id": "lVFkDKsaJbU",
        "view_count": 0
       },
       {
        "channel": "Baby Vishwakarma- vlog ",
        "duration": 25,
        "title": "Look at us, Risha Tara Dilbar Jaani #song #trending #baby",
        "url": "https://www.youtube.com/shorts/q_2osY_CXxo",
        "video_id": "q_2osY_CXxo",
        "view_count": 74
       },
       {
        "channel": "Rahul ki Fun",
        "duration": 19,
        "title": "She Pranked Us Back üòÉüòÑ #entertainment #youtubeshorts #trending#prank #funny #funnyprank",
        "url": "https://www.youtube.com/shorts/XJwlAYXFdNI",
        "video_id": "XJwlAYXFdNI",
        "view_count": 1066
       },
       {
        "channel": "ùö±·èüÍöå ·èü‚ç∫·∫ùùíÜ ‚ô°",
        "duration": 14,
        "title": "usüëÄ #bts #anime #shorts #trending #bmw",
        "url": "https://www.youtube.com/shorts/wJ4gZrmDlYQ",
        "video_id": "wJ4gZrmDlYQ",
        "view_count": 1359
       },
       {
        "channel": "Mickey ",
        "duration": 20,
        "title": "What happened to us üòÇüòÇ #trending #funny#viral #like #shorts @Mickeyy002",
        "url": "https://www.youtube.com/shorts/CMouHRUIVAE",
        "video_id": "CMouHRUIVAE",
        "view_count": 0
       },
       {
        "channel": "Talking ginger kids cat ",
        "duration": 13,
        "title": "POP US TOUS #gameplay #shorts #trending #trendingshorts #like",
        "url": "https://www.youtube.com/shorts/Z-9QX-ANF9U",
        "video_id": "Z-9QX-ANF9U",
        "view_count": 0
       },
       {
        "channel": "MT . artist ",
        "duration": 8,
        "title": "us ki ankhon me #chotaalivlogs #mahadev #trending",
        "url": "https://www.youtube.com/shorts/wtt1a3l2ejw",
        "video_id": "wtt1a3l2ejw",
        "view_count": 656
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trending_videos=get_trending_videos.run(\"US\")\n",
    "# Display as formatted JSON\n",
    "display(JSON(trending_videos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a AI messege! Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, you'll invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's second response. After receiving the video ID, the LLM will likely decide to use another tool to help with the summarization task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_transcript',\n",
       "  'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'},\n",
       "  'id': 'call_og4wR9BLJPPeNFaJKz8pGwa5',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_2 = response_2.tool_calls\n",
    "tool_calls_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the LLM has decided to use the `fetch_transcript` tool as its next step. \n",
    "\n",
    "The model is passing two arguments to the transcript fetching tool:\n",
    "1. `video_id`: 'T-D1OfcDW1M' - The ID that was extracted from the original YouTube URL\n",
    "2. `language`: 'en' - Requesting the transcript in English as specified in the user's query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fetching the transcript using the video ID obtained in the previous step. Here, you're executing the second tool that the LLM requested by:\n",
    "1. Looking up the appropriate function `('fetch_transcript')` from your tool mapping\n",
    "2. Invoking it with the video ID and language parameters\n",
    "3. Storing the resulting transcript content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large language models,\\xa0or LLMs, that generate text in response to a user query, referred to as a prompt. These\\xa0models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most\\xa0moons?\" And my response was, ‚ÄúOh, that\\'s really great that you\\'re asking this question. I loved\\xa0space when I was your age.‚Äù Of course, that was like 30 years ago. But I know this! I read an\\xa0article and the article said that it was Jupiter and 88 moons. So that\\'s the answer. Now, actually,\\xa0there\\'s a couple of things wrong with my answer. First of all, I have no source to support what\\xa0I\\'m saying. So even though I confidently said ‚ÄúI read an article, I know the answer!‚Äù, I\\'m not\\xa0sourcing it. I\\'m giving the answer off the top of my head. And also, I actually haven\\'t kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. They‚Äôre LLM\\xa0challenges. Now, what would have happened if I\\'d taken a beat and first gone and looked\\xa0up the answer on a reputable source like NASA? Well, then I would have been able to say, ‚ÄúAh,\\xa0okay! So the answer is Saturn with 146 moons.‚Äù And in fact, this keeps changing because scientists\\xa0keep on discovering more and more moons. So I have now grounded my answer in something more\\xa0\\nbelievable. I have not hallucinated or made up an answer. Oh, by the way, I didn\\'t leak personal\\xa0information about how long ago it\\'s been since I was obsessed with space. All right, so what does\\xa0this have to do with large language models? Well, how would a large language model have answered\\xa0this question? So let\\'s say that I have a user asking this question about moons. A large language\\xa0model would confidently say, OK, I have been trained and from what I know in my parameters\\xa0during my training, the answer is Jupiter. The answer is wrong. But, you know, we don\\'t know. The large language model is very confident in what it answered. Now, what happens when you add this\\xa0retrieval augmented part here? What does that mean? That means that now, instead of just relying\\xa0on what the LLM knows, we are adding a content store. This could be open like the internet. This\\xa0can be closed like some collection of documents, collection of policies, whatever. The point,\\xa0though, now is that the LLM first goes and talks to the content store and says,\\xa0‚ÄúHey, can you retrieve for me information that is relevant to what the user\\'s\\xa0query was?‚Äù And now, with this retrieval-augmented answer, it\\'s not Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, ‚ÄúOh, okay, I know\\xa0the response. Here it is. Here\\'s my response.‚Äù\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, ‚ÄúI don\\'t know.‚Äù If\\xa0the user\\'s question cannot be reliably answered based on your data store, the model should say,\\xa0\"I don\\'t know,\" instead of making up something that is believable and may mislead the user. This\\xa0can have a negative effect as well though, because if the retriever is not sufficiently\\xa0good to give the large language model the best, most high-quality grounding information, then\\xa0maybe the user\\'s query that is answerable doesn\\'t get an answer. So this is actually why lots\\xa0of folks, including many of us here at IBM, are working the problem on both sides. We are both\\xa0working to improve the retriever to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript_tool_output = tool_mapping[tool_calls_2[0]['name']].invoke(tool_calls_2[0]['args'])\n",
    "fetch_transcript_tool_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You're adding the transcript content to your conversation history by creating another `ToolMessage` that contains the transcript text and the ID of the tool call that requested it. This gives the LLM access to the actual video content so it can generate a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = fetch_transcript_tool_output, tool_call_id = tool_calls_2[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the final summary by sending your complete conversation history to the LLM. Now that the model has access to both the video ID and the full transcript, you'll invoke it one more time to generate the summary that the user requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing large language models (LLMs) and a framework called Retrieval-Augmented Generation (RAG). \\n\\nKey points from the summary:\\n\\n1. **Understanding LLMs**: LLMs generate text in response to user queries but can have issues, such as providing outdated or unsupported answers.\\n\\n2. **Personal Anecdote**: Marina shares a personal story to illustrate the limitations of LLMs. When asked about the planet with the most moons, she mistakenly cites Jupiter as the answer based on outdated information, highlighting the importance of accurate sourcing.\\n\\n3. **The RAG Framework**: RAG enhances LLMs by allowing them to retrieve up-to-date information from a content store before generating responses. This improves accuracy and provides sources for the information given.\\n\\n4. **Improved Accuracy**: By using RAG, LLMs can combine user queries with retrieved data, leading to more precise answers (e.g., correctly stating that Saturn has more moons than Jupiter).\\n\\n5. **Addressing Challenges**: RAG addresses two main issues in LLMs: staying current with information (by updating the content store) and providing verifiable sources for answers, thereby reducing the likelihood of hallucinations or misinformation.\\n\\n6. **Future Work**: Ongoing efforts are being made to enhance both the retrieval of quality information and the generative capabilities of LLMs to serve users better.\\n\\nOverall, the video emphasizes the potential of RAG to create more reliable and trustworthy AI systems in generating responses to user queries.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 323, 'prompt_tokens': 1889, 'total_tokens': 2212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZzrNf2DzR68OxEbXZZ4rcZFgopzL', 'finish_reason': 'stop', 'logprobs': None}, id='run--f047e27b-27b8-4dcf-b3ce-7807c5e975ae-0', usage_metadata={'input_tokens': 1889, 'output_tokens': 323, 'total_tokens': 2212, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the tool calling process\n",
    "\n",
    "You manually saw how you input a text request to your LLM, where the LLM recognized that a tool call was required. Then, you extracted the tool content, formatted the input, made the next tool call, and repeated these steps. While this step-by-step approach helps understand the process, it would be tedious to implement for every application. Now let's automate this entire workflow.\n",
    "\n",
    "#### Extracting tool information from LLM response\n",
    "Create a function to automate tool calling. The input is the tool call object from which you extract the name, and use the tool_mapping dictionary to find the correct function to call. You'll pass the arguments from the tool call to this function and then send the output back as a ToolMessage with the tool_call_id included.\n",
    "The tool_call_id is an essential part of this process as it links each tool response back to the specific tool request made by the language model. This ID ensures the LLM can match responses to its requests, which is crucial when multiple tools are called in sequence or simultaneously. Without this ID, the LLM would have no way to know which response corresponds to which request, making multi-step reasoning impossible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing steps\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ToolMessage(\n",
    "            content=f\"Error: {str(e)}\",\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to chain all your functions or tools together, but before you do so, you need to format the data properly. Not only are you required to store the output of each tool, but you also need to store state information like tool IDs. To do this effectively, you must ensure the output of each tool can be properly passed to the next step in your pipeline. The RunnablePassthrough component allows you to maintain state throughout the chain while adding or transforming data at each step, making it ideal for connecting your various tools into a cohesive workflow.\n",
    "The RunnableLambda, placed at the end of your chain, serves a different purpose - it extracts only the final result you want to present to the user. After all the tool calls and message processing, you have a rich state object with many fields, but the user typically only needs the final answer. The RunnableLambda transforms this complete state into just the information you want to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the summarization chain\n",
    "\n",
    "Now, you'll combine your functions into a complete `summarization_chain` using the pipe operator `|`, which applies functions sequentially (similar to function composition where `f|g(x)` is equivalent to `f(g(x))`).\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. Convert the input prompt to a HumanMessage\n",
    "2. Pass the message to LLM with tools\n",
    "3. Extract tool calls from LLM response\n",
    "4. Update message history with tool results\n",
    "5. Send updated messages back to LLM\n",
    "6. Repeat steps 3-5 as needed\n",
    "7. Finally, extract just the content from the final message using RunnableLambda\n",
    "\n",
    "Each step maintains state using RunnablePassthrough until you reach the final message, at which point you'll apply RunnableLambda to extract only the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_chain = (\n",
    "    # Start with initial query\n",
    "    RunnablePassthrough.assign(\n",
    "        messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    "    )\n",
    "    # First LLM call (extract video ID)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process first tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Update message history\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    "    )\n",
    "    # Second LLM call (fetch transcript)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process second tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages2=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Final message update\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    "    )\n",
    "    # Generate final summary\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    "    )\n",
    "    # Return just the summary text\n",
    "    | RunnableLambda(lambda x: x[\"summary\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you invoke the summarization chain with a YouTube video URL; this passes your query containing a YouTube URL to the chain, which automatically extracts the video ID, fetches the transcript, and generates a summary of the content.\n",
    "\n",
    "**Note: If you find any issues with the given video link below, try any Youtube video link of your choosing.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The video titled \"A Day in the Life at American Public School\" takes the viewer through a typical school day from the perspective of a student. It starts with the student biking to school, experiencing cold weather, and highlighting the arrival on the school bus. The transcript includes snippets of conversations with friends about various topics, such as fashion, American history class, and school lunch.\n",
      "\n",
      "Throughout the day, the student attends different classes, interacts with peers, and shares insights into the school environment. They mention taking tests, working on school projects, and enjoying lunch, which they criticize for being unappetizing. The student also reflects on their friends as the best part of school, while noting the pressures of academic expectations as a downside.\n",
      "\n",
      "As the day progresses, the student captures moments of camaraderie with friends, showcases school activities, and concludes with a humorous encounter that emphasizes the light-hearted nature of school life. The overall tone is casual and relatable, focusing on the experience of being a high school student in the U.S.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "result = summarization_chain.invoke({\n",
    "    \"query\": \"Summarize this YouTube video: https://www.youtube.com/watch?v=FGvdNUgC5Kk\"\n",
    "})\n",
    "\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Up to this point, you've demonstrated how to manually orchestrate the tool calling process step by step. You first invoked the LLM with the user's query, interpreted its decision to use the `extract_video_id` tool, executed that tool, fed the result back to the LLM, processed its next decision to use the `fetch_transcript` tool, executed that tool, and finally had the LLM generate a summary based on the transcript.\n",
    "\n",
    "Now you'll see how to accomplish the same workflow more efficiently using LangChain's chain functionality, which automates this back-and-forth process of tool selection, execution, and response handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the initial message setup\n",
    "\n",
    "Here you're setting up the first step of your chain that will handle the initial user query. The `RunnablePassthrough.assign` creates a component that takes an input dictionary containing a \"query\" and converts it into a list containing a single `HumanMessage` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_setup = RunnablePassthrough.assign(\n",
    "    messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the first LLM interaction\n",
    "\n",
    "Here, you'll create the second step of your chain, which handles the first interaction with the language model. This component takes the formatted messages from the previous step, sends them to your tool-equipped LLM, and captures the response in a field called \"ai_response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the first tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's first tool call. This component:\n",
    "1. Executes each tool call by passing it to your `execute_tool` function, which runs the appropriate tool and returns the result as a `ToolMessage`\n",
    "2. Updates the message history by combining the original messages, the LLM's response, with the tool calls, and the tool results\n",
    "3. Prepares the updated conversation state for the next interaction with the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the second LLM interaction\n",
    "\n",
    "Here, you're creating the next step in your chain that handles the second interaction with the language model. This component takes the updated message history (which now includes the results from the first tool call) and sends it to the LLM again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the second tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's second tool call. Similar to the first tool processing step, this component executes the tool calls (typically fetching the transcript), creates tool messages with the results, and updates the message history by combining everything for the final summarization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages2=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the final summary\n",
    "\n",
    "Here, you're defining the final step that produces the summary of the YouTube video. This component:\n",
    "1. Takes the complete message history (which now contains the original query, tool calls, and tool results)\n",
    "2. Invokes the LLM one last time to generate a summary\n",
    "3. Extracts just the content field from the LLM's response\n",
    "4. Uses a RunnableLambda to return only the summary text as the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  summary: RunnableLambda(lambda x: llm_with_tools.invoke(x['messages']).content)\n",
       "})\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary = RunnablePassthrough.assign(\n",
    "    summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    ") | RunnableLambda(lambda x: x[\"summary\"])\n",
    "\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembling the complete chain\n",
    "\n",
    "Now, you're combining all the individual components you've defined into a single cohesive chain. By piping each step to the next, you'll create a workflow that:\n",
    "1. Formats the initial query\n",
    "2. Gets the first LLM response (video ID extraction)\n",
    "3. Processes the first tool call\n",
    "4. Gets the second LLM response (transcript request)\n",
    "5. Processes the second tool call\n",
    "6. Generates the final summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    initial_setup\n",
    "    | first_llm_call\n",
    "    | first_tool_processing\n",
    "    | second_llm_call\n",
    "    | second_tool_processing\n",
    "    | final_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you're testing your automated chain with the original video summarization query you handled manually before. By passing in the same query to your chain, you can confirm that it produces the same results but in a much more streamlined manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing a framework called Retrieval-Augmented Generation (RAG) designed to enhance the accuracy and timeliness of large language models (LLMs). Here's a summary:\n",
      "\n",
      "1. **Introduction to LLMs**: Danilevsky highlights that while LLMs can generate impressive text responses, they often exhibit undesirable behaviors, such as providing inaccurate or outdated information without proper sourcing.\n",
      "\n",
      "2. **The Problem**: She shares an anecdote about a question her children asked regarding the planet with the most moons, revealing how her confident yet outdated answer (Jupiter) lacked a reliable source.\n",
      "\n",
      "3. **RAG Framework**: RAG improves LLM responses by incorporating a content retrieval system. Instead of solely relying on the training data, LLMs can now first fetch relevant and current information from a content store (which could be open like the internet or closed like specific documents) before generating a response.\n",
      "\n",
      "4. **Benefits of RAG**:\n",
      "   - **Up-to-Date Information**: Users can receive accurate answers without the need for retraining the model; simply updating the information in the data store suffices.\n",
      "   - **Citing Sources**: The model can now provide evidence for its responses, which reduces the chance of fabricating information and allows it to confidently state when it does not know the answer.\n",
      "\n",
      "5. **Challenges**: There are potential downsides if the retrieval system fails to provide high-quality information, which could prevent the LLM from answering valid questions.\n",
      "\n",
      "6. **Conclusion**: Danilevsky emphasizes the importance of both improving the retrieval system and the generative model to enhance the quality of interactions with users.\n",
      "\n",
      "Overall, RAG presents a method to increase the reliability and relevance of information generated by large language models.\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Chain with a Different Query\n",
    "\n",
    "Here, you're testing your completed chain with a new query to demonstrate its flexibility. Instead of requesting a video summary, you're asking for information about trending videos in India. You'll create a dictionary with the query and invoke your chain, which will handle all the necessary tool calls automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842ba1310>, 'Connection to rr2---sn-p5qlsny6.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842bd3f80>, 'Connection to rr2---sn-p5qlsny6.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842bd3380>, 'Connection to rr2---sn-p5qlsny6.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab8420f2e70>, 'Connection to rr5---sn-p5qs7nzr.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab8420f31a0>, 'Connection to rr5---sn-p5qs7nzr.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841c36ae0>, 'Connection to rr2---sn-p5qlsn7d.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841c64a40>, 'Connection to rr2---sn-p5qlsn7d.googlevideo.com timed out. (connect timeout=20.0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " Here are the top 3 trending YouTube videos in Indonesia along with their metadata:\n",
      "\n",
      "### 1. [id hack #shorts #trending #freefire #viralshorts #youtubeshorts #popularshorts #freefireshorts #ff](https://www.youtube.com/shorts/lQyj8x9nZ5w)\n",
      "- **Channel:** SIKARI BHAI\n",
      "- **Views:** 0\n",
      "- **Duration:** 16 seconds\n",
      "- **Likes:** Not available\n",
      "- **Comments:** Not available\n",
      "- **Chapters:** Not available\n",
      "\n",
      "---\n",
      "\n",
      "### 2. [ü•∞üò¢‚ù§Ô∏è‚Äçü©π#trending sad song my channel ID ko subscribe kare please üôè](https://www.youtube.com/shorts/IRR09-fxP_c)\n",
      "- **Channel:** Aman___up___ 42\n",
      "- **Views:** 1\n",
      "- **Duration:** 15 seconds\n",
      "- **Likes:** 0\n",
      "- **Comments:** Not available\n",
      "- **Chapters:** Not available\n",
      "\n",
      "---\n",
      "\n",
      "### 3. [THIS FOR MY ID üóø #trending #freefire #totalgaming](https://www.youtube.com/shorts/KB9egJJyQYU)\n",
      "- **Channel:** Abhi_Adiwal_Gaming\n",
      "- **Views:** 2\n",
      "- **Duration:** 14 seconds\n",
      "- **Likes:** 1\n",
      "- **Comments:** Not available\n",
      "- **Chapters:** Not available\n",
      "\n",
      "--- \n",
      "\n",
      "Please note that the view counts are quite low as these are trending shorts, and statistics may update frequently.\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"Get top 3 youtube videos in Indonesia and their metadata\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are the top 3 trending YouTube videos in Indonesia along with their metadata:\\n\\n### 1. [id hack #shorts #trending #freefire #viralshorts #youtubeshorts #popularshorts #freefireshorts #ff](https://www.youtube.com/shorts/lQyj8x9nZ5w)\\n- **Channel:** SIKARI BHAI\\n- **Views:** 0\\n- **Duration:** 16 seconds\\n- **Likes:** Not available\\n- **Comments:** Not available\\n- **Chapters:** Not available\\n\\n---\\n\\n### 2. [ü•∞üò¢‚ù§Ô∏è\\u200dü©π#trending sad song my channel ID ko subscribe kare please üôè](https://www.youtube.com/shorts/IRR09-fxP_c)\\n- **Channel:** Aman___up___ 42\\n- **Views:** 1\\n- **Duration:** 15 seconds\\n- **Likes:** 0\\n- **Comments:** Not available\\n- **Chapters:** Not available\\n\\n---\\n\\n### 3. [THIS FOR MY ID üóø #trending #freefire #totalgaming](https://www.youtube.com/shorts/KB9egJJyQYU)\\n- **Channel:** Abhi_Adiwal_Gaming\\n- **Views:** 2\\n- **Duration:** 14 seconds\\n- **Likes:** 1\\n- **Comments:** Not available\\n- **Chapters:** Not available\\n\\n--- \\n\\nPlease note that the view counts are quite low as these are trending shorts, and statistics may update frequently.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive chain flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created a chain that works well for your specific two-step tool calling process, you need to consider more complex scenarios. Your current chain is limited to exactly two tool calls in a fixed sequence. In real-world applications, you might need a variable number of tool calls depending on the user's query - for example, getting trending videos and then fetching metadata for each video, or searching for videos on a topic and then getting transcripts for multiple results.\n",
    "\n",
    "To handle these more complex scenarios, you'll build a recursive chain that can dynamically decide how many tool calls are needed and continue processing until all necessary information has been gathered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "import json\n",
    "\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        content = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
    "    except Exception as e:\n",
    "        content = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ToolMessage(\n",
    "        content=content,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the core processing logic\n",
    "\n",
    "This function handles the core processing logic of your recursive chain. It takes the current conversation history and:\n",
    "\n",
    "1. Identifies the most recent message in the conversation\n",
    "2. Extracts all tool calls from that message and executes them in parallel using your `execute_tool` helper\n",
    "3. Updates the message history by adding the tool response messages\n",
    "4. Gets the next response from the language model based on the updated conversation\n",
    "5. Returns the complete updated message history with both tool responses and the new LLM response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tool_calls(messages):\n",
    "    \"\"\"Recursive tool call processor\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Execute all tool calls in parallel\n",
    "    tool_messages = [\n",
    "        execute_tool(tc) \n",
    "        for tc in getattr(last_message, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # Add tool responses to message history\n",
    "    updated_messages = messages + tool_messages\n",
    "    \n",
    "    # Get next LLM response\n",
    "    next_ai_response = llm_with_tools.invoke(updated_messages)\n",
    "    \n",
    "    return updated_messages + [next_ai_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the recursive stopping condition\n",
    "\n",
    "This function determines whether your recursive process should continue or terminate. It:\n",
    "\n",
    "1. Takes the current message history and examines the last message\n",
    "2. Checks if that message contains any tool calls using the `getattr` function (which safely handles cases where the attribute might not exist)\n",
    "3. Returns a boolean value - `True` if there are more tool calls to process, and `False` when you reach a point where the LLM has provided a final answer without requesting additional tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(messages):\n",
    "    \"\"\"Check if you need another iteration\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    return bool(getattr(last_message, 'tool_calls', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementing the recursive function\n",
    "\n",
    "This function implements the actual recursion that powers your dynamic tool calling process:\n",
    "\n",
    "1. It first checks the stopping condition using the `should_continue` function to determine if more tools need to be called\n",
    "2. If more tool calls are needed, it processes those calls using your `process_tool_calls` function and then recursively calls itself with the updated messages\n",
    "3. If no more tool calls are needed, it returns the final message history, which contains the complete conversation, including the LLM's final response\n",
    "\n",
    "After defining this recursive function, you'll wrap it in a `RunnableLambda` to make it compatible with LangChain's chain architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_chain(messages):\n",
    "    \"\"\"Recursively process tool calls until completion\"\"\"\n",
    "    if should_continue(messages):\n",
    "        new_messages = process_tool_calls(messages)\n",
    "        return _recursive_chain(new_messages)\n",
    "    return messages\n",
    "\n",
    "recursive_chain = RunnableLambda(_recursive_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the complete universal chain\n",
    "\n",
    "Now, you're assembling your final universal chain that can handle any type of query requiring any number of tool calls. This chain consists of three main steps:\n",
    "\n",
    "1. The first step converts the user query into a properly formatted `HumanMessage` object\n",
    "2. The second step sends this initial message to your tool-equipped LLM and adds the LLM's first response to the message history\n",
    "3. The final step passes the conversation to your recursive chain, which will handle all subsequent tool calls until the LLM provides a final answer\n",
    "\n",
    "This universal chain is much more flexible than your earlier fixed-step chain, as it can dynamically adapt to queries that require different numbers and types of tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_chain = (\n",
    "    RunnableLambda(lambda x: [HumanMessage(content=x[\"query\"])])\n",
    "    | RunnableLambda(lambda messages: messages + [llm_with_tools.invoke(messages)])\n",
    "    | recursive_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842aa3500>, 'Connection to rr5---sn-p5qlsndk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842bd2a80>, 'Connection to rr5---sn-p5qlsndk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab84316d6a0>, 'Connection to rr1---sn-p5qlsn7s.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab843159790>, 'Connection to rr1---sn-p5qlsn7s.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab84338a2a0>, 'Connection to rr1---sn-p5qlsn7s.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab842b04b60>, 'Connection to rr3---sn-p5qddn7k.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab84202bf20>, 'Connection to rr3---sn-p5qddn7k.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab84202a9f0>, 'Connection to rr3---sn-p5qddn7k.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab8421970e0>, 'Connection to rr5---sn-p5qlsndk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab8421c9f70>, 'Connection to rr5---sn-p5qlsndk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841638d70>, 'Connection to rr5---sn-p5qs7nsk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab84163a780>, 'Connection to rr5---sn-p5qs7nsk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab8416f8500>, 'Connection to rr5---sn-p5qs7nsk.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841513800>, 'Connection to rr2---sn-p5qlsn7l.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841379a90>, 'Connection to rr2---sn-p5qlsn7l.googlevideo.com timed out. (connect timeout=20.0)')\n",
      "[download] Got error: (<urllib3.connection.HTTPSConnection object at 0x7ab841378d70>, 'Connection to rr2---sn-p5qlsn7l.googlevideo.com timed out. (connect timeout=20.0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are the top 3 trending videos in the US, including metadata and thumbnails:\\n\\n### 1. [Dua said, ‚ÄúPeople want us to compete‚Ä¶but the truth #dualipa #trending #shortsvideo](https://www.youtube.com/shorts/PitOVyLdT7w) \\n- **Channel:** God Says 11:11\\n- **Views:** 80\\n- **Duration:** 4 seconds\\n- **Likes:** 0\\n- **Comments:** Not available\\n\\n**Thumbnail:**\\n![Dua said, ‚ÄúPeople want us to compete‚Ä¶but the truth #dualipa #trending #shortsvideo](https://i.ytimg.com/vi/PitOVyLdT7w/maxres2.jpg?sqp=-oaymwEoCIAKENAF8quKqQMcGADwAQH4Ac4FgAKACooCDAgAEAEYIiBgKH8wDw==&rs=AOn4CLCLH9DGkXsgnvMJN8CDg8kW9dGT-w)\\n\\n---\\n\\n### 2. [#hiphop #rap #music #trending #freestyle #trendingshorts #trend#us #usa #canada #france](https://www.youtube.com/shorts/xNU_BOOqqQw) \\n- **Channel:** Md Qamar Raza\\n- **Views:** 4\\n- **Duration:** 46 seconds\\n- **Likes:** 1\\n- **Comments:** Not available\\n\\n**Thumbnail:**\\n![#hiphop #rap #music #trending #freestyle #trendingshorts #trend#us #usa #canada #france](https://i.ytimg.com/vi/xNU_BOOqqQw/maxres2.jpg?sqp=-oaymwEoCIAKENAF8quKqQMcGADwAQH4AbYIgAKAD4oCDAgAEAEYciBGKDEwDw==&rs=AOn4CLBtYw1E57g_bgI7d8YyY_sX4eiJHg)\\n\\n---\\n\\n### 3. [us Gym workout üí™#shorts #shortsviral #trending #shortsfeed](https://www.youtube.com/shorts/GgZj1JlVQN8) \\n- **Channel:** Harshal_GREEN SCREEN \\n- **Views:** 4\\n- **Duration:** 15 seconds\\n- **Likes:** 0\\n- **Comments:** Not available\\n\\n**Thumbnail:**\\n![us Gym workout üí™#shorts #shortsviral #trending #shortsfeed](https://i.ytimg.com/vi/GgZj1JlVQN8/maxresdefault.jpg?sqp=-oaymwEoCIAKENAF8quKqQMcGADwAQH4AbYIgAKAD4oCDAgAEAEYZSBdKFMwDw==&rs=AOn4CLA_Z2Abzvy6j55FyR7eqQcWKO_NqA)\\n\\n--- \\n\\nFeel free to click on the titles for direct access to the videos!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 656, 'prompt_tokens': 12936, 'total_tokens': 13592, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-Ca00UliPzcujyE6IO6FfKa6VOMOPy', 'finish_reason': 'stop', 'logprobs': None} id='run--3e8234c4-9848-4f09-862f-c33e2c9a0264-0' usage_metadata={'input_tokens': 12936, 'output_tokens': 656, 'total_tokens': 13592, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(universal_chain.invoke({\n",
    "    \"query\": \"Show top 3 US trending videos with metadata and thumbnails\"\n",
    "})[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a different video with a Youtube link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "youtube_url = 'https://www.youtube.com/watch?v=8z6wh3ILoHM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = \"INSERT_VIDEO_ID_HERE\"  # Replace with the actual video ID\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Extract the video ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted video ID: 8z6wh3ILoHM\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Collect all necessary data about the video in one go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved metadata for: Why should we drink more water? | üéôÔ∏è 8 Minute English\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Get video transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved transcript with 6194 characters\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Get video thumbnails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 46 thumbnails\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video thumbnails\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a comprehensive prompt to be passed to LLM to generate a summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for prompt</summary>\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Single LLM invocation with all the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Display the comprehensive analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VIDEO ANALYSIS =====\n",
      "\n",
      "### Summary of Video Content\n",
      "1. **Importance of Hydration**: The video discusses the critical health benefits of drinking water over sugary sodas, especially after physical activities.\n",
      "2. **Signs of Dehydration**: It explains common signs of dehydration such as dry mouth, fatigue, headaches, and dark-colored urine.\n",
      "3. **Daily Water Intake Recommendations**: The video suggests daily water intake levels, approximately 3.7 liters for men and 2.7 liters for women, increasing with physical activity.\n",
      "4. **Vocabulary Development**: The hosts introduce relevant vocabulary such as \"quench,\" \"thirst,\" \"dehydration,\" and \"urine\" to aid English learners.\n",
      "5. **Engaging Presentation**: The conversational format between the hosts makes the topic relatable while providing tips to improve English language skills.\n",
      "\n",
      "### Main Topics or Themes Discussed\n",
      "- Health and well-being benefits of drinking water\n",
      "- Dehydration and its signs\n",
      "- Daily water consumption guidelines based on individual factors\n",
      "- Vocabulary enhancement related to hydration and health\n",
      "\n",
      "### Intended Audience for This Content\n",
      "The intended audience appears to be English language learners who are interested in improving their English skills while also learning about health topics. This includes both beginner and intermediate learners, as the dialogue and vocabulary are accessible yet informative.\n",
      "\n",
      "### Analysis of Video Performance\n",
      "- **Engaging Format**: The conversational format between two friends discussing relatable experiences makes it engaging for viewers.\n",
      "- **Relevant Topic**: Health and hydration are universally relevant topics, making the content appealing to a broad audience.\n",
      "- **Educational Value**: It offers educational content alongside language learning, which may attract viewers looking for practical information to enhance both their health and English skills.\n",
      "- **High Engagement Metrics**: With over 222,000 views and more than 8,500 likes, the video suggests that there is significant interest in the subject matter, likely boosted by its informative nature and clear communication.\n",
      "\n",
      "Overall, these factors point to a successful video that effectively meets the needs of its target audience while promoting health awareness and language learning.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana) is a Data Scientist at IBM and is currently pursuing his Master's in Computer Science at Dalhousie University.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ¬© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "1603f145970a8c2283bae36941000a6d4b9e191c2755a7412e95925fa9d1ae18"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
